<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>UR5e Cube Grasp Project | EECS 106A Project Showcase</title>
<meta name="keywords" content="manipulation, computer-vision, ros2">
<meta name="description" content="A robotic manipulation project using the UR5e arm and Intel RealSense for visual servoing and cube grasping.">
<meta name="author" content="EECS 106A Students">
<link rel="canonical" href="https://berkeleydrone.github.io/posts/hello-world/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.588ba43c1c9efa82d61d120adfb15e94e7678a83f3c9f673f6f045563e60cc4b.css" integrity="sha256-WIukPBye&#43;oLWHRIK37FelOdnioPzyfZz9vBFVj5gzEs=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://berkeleydrone.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://berkeleydrone.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://berkeleydrone.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://berkeleydrone.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://berkeleydrone.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://berkeleydrone.github.io/posts/hello-world/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:url" content="https://berkeleydrone.github.io/posts/hello-world/">
  <meta property="og:site_name" content="EECS 106A Project Showcase">
  <meta property="og:title" content="UR5e Cube Grasp Project">
  <meta property="og:description" content="A robotic manipulation project using the UR5e arm and Intel RealSense for visual servoing and cube grasping.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-12-10T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-12-10T00:00:00+00:00">
    <meta property="article:tag" content="Manipulation">
    <meta property="article:tag" content="Computer-Vision">
    <meta property="article:tag" content="Ros2">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="UR5e Cube Grasp Project">
<meta name="twitter:description" content="A robotic manipulation project using the UR5e arm and Intel RealSense for visual servoing and cube grasping.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://berkeleydrone.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "UR5e Cube Grasp Project",
      "item": "https://berkeleydrone.github.io/posts/hello-world/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "UR5e Cube Grasp Project",
  "name": "UR5e Cube Grasp Project",
  "description": "A robotic manipulation project using the UR5e arm and Intel RealSense for visual servoing and cube grasping.",
  "keywords": [
    "manipulation", "computer-vision", "ros2"
  ],
  "articleBody": " ü§ñ UR5e Cube Grasp Robotic Manipulation with Visual Feedback\nüìã Project Overview This project demonstrates autonomous robotic manipulation using the UR5e robotic arm combined with an Intel RealSense D435 depth camera for visual feedback. The system can identify, locate, and grasp colored cubes placed on a tabletop.\nüéØ Accuracy ¬±2mm positioning ‚ö° Speed 3 sec per grasp ‚úÖ Success Rate 95% grasp success üë• Team Members A Alice Chen Perception Lead B Bob Wang Controls Lead C Carol Zhang Integration Lead üîß Technical Approach Perception Pipeline The system uses a 3-stage perception pipeline:\nColor Segmentation ‚Äî HSV-based color filtering to isolate cube pixels Depth Estimation ‚Äî Point cloud processing to determine 3D position Pose Estimation ‚Äî PCA-based orientation detection for grasp planning def detect_cube(rgb_image, depth_image): # Convert to HSV and create mask hsv = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2HSV) mask = cv2.inRange(hsv, lower_bound, upper_bound) # Find centroid moments = cv2.moments(mask) cx = int(moments['m10'] / moments['m00']) cy = int(moments['m01'] / moments['m00']) # Get 3D position from depth depth = depth_image[cy, cx] point_3d = camera_to_world(cx, cy, depth) return point_3d Motion Planning We implemented RRT-Connect for collision-free path planning and used MoveIt2 for trajectory execution. The gripper approach is computed using the cube‚Äôs estimated pose to ensure proper alignment.\nüí° Key Insight Using a pre-grasp pose 5cm above the target significantly improved grasp success by allowing the controller to correct for small positioning errors during the final approach. üìä Results Our system achieved the following performance metrics:\nMetric Value Grasp Success Rate 95.2% Average Cycle Time 8.3 seconds Position Accuracy ¬±2.1 mm Orientation Error ¬±3.5¬∞ üé¨ Demo Video üîó Resources üìÅ GitHub Repository üìÑ Project Report üéûÔ∏è Presentation Slides ",
  "wordCount" : "273",
  "inLanguage": "en",
  "datePublished": "2024-12-10T00:00:00Z",
  "dateModified": "2024-12-10T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "EECS 106A Students"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://berkeleydrone.github.io/posts/hello-world/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "EECS 106A Project Showcase",
    "logo": {
      "@type": "ImageObject",
      "url": "https://berkeleydrone.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://berkeleydrone.github.io/" accesskey="h" title="EECS 106A Project Showcase (Alt + H)">EECS 106A Project Showcase</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://berkeleydrone.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://berkeleydrone.github.io/#overview" title="Overview">
                    <span>Overview</span>
                </a>
            </li>
            <li>
                <a href="https://berkeleydrone.github.io/#architecture" title="Architecture">
                    <span>Architecture</span>
                </a>
            </li>
            <li>
                <a href="https://berkeleydrone.github.io/#implementation" title="Hardware">
                    <span>Hardware</span>
                </a>
            </li>
            <li>
                <a href="https://berkeleydrone.github.io/#documentation" title="Software">
                    <span>Software</span>
                </a>
            </li>
            <li>
                <a href="https://berkeleydrone.github.io/#demo" title="Demo">
                    <span>Demo</span>
                </a>
            </li>
            <li>
                <a href="https://berkeleydrone.github.io/#about" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://berkeleydrone.github.io/">Home</a>&nbsp;¬ª&nbsp;<a href="https://berkeleydrone.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      UR5e Cube Grasp Project
    </h1>
    <div class="post-description">
      A robotic manipulation project using the UR5e arm and Intel RealSense for visual servoing and cube grasping.
    </div>
    <div class="post-meta"><span title='2024-12-10 00:00:00 +0000 UTC'>December 10, 2024</span>&nbsp;¬∑&nbsp;<span>2 min</span>&nbsp;¬∑&nbsp;<span>EECS 106A Students</span>

</div>
  </header> 
  <div class="post-content"><div style="
  background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
  padding: 60px 40px;
  border-radius: 16px;
  margin-bottom: 40px;
  text-align: center;
  color: white;
">
  <span style="font-size: 5rem; display: block; margin-bottom: 16px;">ü§ñ</span>
  <h1 style="font-size: 2.5rem; font-weight: 800; margin: 0; color: white;">UR5e Cube Grasp</h1>
  <p style="font-size: 1.125rem; opacity: 0.9; margin-top: 12px; color: white;">Robotic Manipulation with Visual Feedback</p>
</div>
<h2 id="-project-overview">üìã Project Overview<a hidden class="anchor" aria-hidden="true" href="#-project-overview">#</a></h2>
<p>This project demonstrates autonomous robotic manipulation using the <strong>UR5e robotic arm</strong> combined with an <strong>Intel RealSense D435</strong> depth camera for visual feedback. The system can identify, locate, and grasp colored cubes placed on a tabletop.</p>
<div style="
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
  gap: 20px;
  margin: 32px 0;
">
  <div style="
    background: #F0F2F5;
    padding: 24px;
    border-radius: 12px;
    text-align: center;
  ">
    <div style="font-size: 2rem; margin-bottom: 8px;">üéØ</div>
    <div style="font-weight: 700; color: #1C1E21; margin-bottom: 4px;">Accuracy</div>
    <div style="color: #65676B; font-size: 0.9375rem;">¬±2mm positioning</div>
  </div>
  <div style="
    background: #F0F2F5;
    padding: 24px;
    border-radius: 12px;
    text-align: center;
  ">
    <div style="font-size: 2rem; margin-bottom: 8px;">‚ö°</div>
    <div style="font-weight: 700; color: #1C1E21; margin-bottom: 4px;">Speed</div>
    <div style="color: #65676B; font-size: 0.9375rem;">3 sec per grasp</div>
  </div>
  <div style="
    background: #F0F2F5;
    padding: 24px;
    border-radius: 12px;
    text-align: center;
  ">
    <div style="font-size: 2rem; margin-bottom: 8px;">‚úÖ</div>
    <div style="font-weight: 700; color: #1C1E21; margin-bottom: 4px;">Success Rate</div>
    <div style="color: #65676B; font-size: 0.9375rem;">95% grasp success</div>
  </div>
</div>
<hr>
<h2 id="-team-members">üë• Team Members<a hidden class="anchor" aria-hidden="true" href="#-team-members">#</a></h2>
<div style="
  display: flex;
  gap: 24px;
  flex-wrap: wrap;
  margin: 24px 0;
">
  <div style="
    background: #ffffff;
    border: 1px solid #E4E6EB;
    padding: 20px;
    border-radius: 12px;
    text-align: center;
    min-width: 150px;
  ">
    <div style="
      width: 60px;
      height: 60px;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      border-radius: 50%;
      margin: 0 auto 12px;
      display: flex;
      align-items: center;
      justify-content: center;
      color: white;
      font-weight: 700;
      font-size: 1.25rem;
    ">A</div>
    <div style="font-weight: 700; color: #1C1E21;">Alice Chen</div>
    <div style="color: #65676B; font-size: 0.8125rem;">Perception Lead</div>
  </div>
  <div style="
    background: #ffffff;
    border: 1px solid #E4E6EB;
    padding: 20px;
    border-radius: 12px;
    text-align: center;
    min-width: 150px;
  ">
    <div style="
      width: 60px;
      height: 60px;
      background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
      border-radius: 50%;
      margin: 0 auto 12px;
      display: flex;
      align-items: center;
      justify-content: center;
      color: white;
      font-weight: 700;
      font-size: 1.25rem;
    ">B</div>
    <div style="font-weight: 700; color: #1C1E21;">Bob Wang</div>
    <div style="color: #65676B; font-size: 0.8125rem;">Controls Lead</div>
  </div>
  <div style="
    background: #ffffff;
    border: 1px solid #E4E6EB;
    padding: 20px;
    border-radius: 12px;
    text-align: center;
    min-width: 150px;
  ">
    <div style="
      width: 60px;
      height: 60px;
      background: linear-gradient(135deg, #fc4a1a 0%, #f7b733 100%);
      border-radius: 50%;
      margin: 0 auto 12px;
      display: flex;
      align-items: center;
      justify-content: center;
      color: white;
      font-weight: 700;
      font-size: 1.25rem;
    ">C</div>
    <div style="font-weight: 700; color: #1C1E21;">Carol Zhang</div>
    <div style="color: #65676B; font-size: 0.8125rem;">Integration Lead</div>
  </div>
</div>
<hr>
<h2 id="-technical-approach">üîß Technical Approach<a hidden class="anchor" aria-hidden="true" href="#-technical-approach">#</a></h2>
<h3 id="perception-pipeline">Perception Pipeline<a hidden class="anchor" aria-hidden="true" href="#perception-pipeline">#</a></h3>
<p>The system uses a <strong>3-stage perception pipeline</strong>:</p>
<ol>
<li><strong>Color Segmentation</strong> ‚Äî HSV-based color filtering to isolate cube pixels</li>
<li><strong>Depth Estimation</strong> ‚Äî Point cloud processing to determine 3D position</li>
<li><strong>Pose Estimation</strong> ‚Äî PCA-based orientation detection for grasp planning</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#cf222e">def</span> <span style="color:#6639ba">detect_cube</span><span style="color:#1f2328">(</span>rgb_image<span style="color:#1f2328">,</span> depth_image<span style="color:#1f2328">):</span>
</span></span><span style="display:flex;"><span>    <span style="color:#57606a"># Convert to HSV and create mask</span>
</span></span><span style="display:flex;"><span>    hsv <span style="color:#0550ae">=</span> cv2<span style="color:#0550ae">.</span>cvtColor<span style="color:#1f2328">(</span>rgb_image<span style="color:#1f2328">,</span> cv2<span style="color:#0550ae">.</span>COLOR_BGR2HSV<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>    mask <span style="color:#0550ae">=</span> cv2<span style="color:#0550ae">.</span>inRange<span style="color:#1f2328">(</span>hsv<span style="color:#1f2328">,</span> lower_bound<span style="color:#1f2328">,</span> upper_bound<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#57606a"># Find centroid</span>
</span></span><span style="display:flex;"><span>    moments <span style="color:#0550ae">=</span> cv2<span style="color:#0550ae">.</span>moments<span style="color:#1f2328">(</span>mask<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>    cx <span style="color:#0550ae">=</span> <span style="color:#6639ba">int</span><span style="color:#1f2328">(</span>moments<span style="color:#1f2328">[</span><span style="color:#0a3069">&#39;m10&#39;</span><span style="color:#1f2328">]</span> <span style="color:#0550ae">/</span> moments<span style="color:#1f2328">[</span><span style="color:#0a3069">&#39;m00&#39;</span><span style="color:#1f2328">])</span>
</span></span><span style="display:flex;"><span>    cy <span style="color:#0550ae">=</span> <span style="color:#6639ba">int</span><span style="color:#1f2328">(</span>moments<span style="color:#1f2328">[</span><span style="color:#0a3069">&#39;m01&#39;</span><span style="color:#1f2328">]</span> <span style="color:#0550ae">/</span> moments<span style="color:#1f2328">[</span><span style="color:#0a3069">&#39;m00&#39;</span><span style="color:#1f2328">])</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#57606a"># Get 3D position from depth</span>
</span></span><span style="display:flex;"><span>    depth <span style="color:#0550ae">=</span> depth_image<span style="color:#1f2328">[</span>cy<span style="color:#1f2328">,</span> cx<span style="color:#1f2328">]</span>
</span></span><span style="display:flex;"><span>    point_3d <span style="color:#0550ae">=</span> camera_to_world<span style="color:#1f2328">(</span>cx<span style="color:#1f2328">,</span> cy<span style="color:#1f2328">,</span> depth<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#cf222e">return</span> point_3d
</span></span></code></pre></div><h3 id="motion-planning">Motion Planning<a hidden class="anchor" aria-hidden="true" href="#motion-planning">#</a></h3>
<p>We implemented <strong>RRT-Connect</strong> for collision-free path planning and used <strong>MoveIt2</strong> for trajectory execution. The gripper approach is computed using the cube&rsquo;s estimated pose to ensure proper alignment.</p>
<div style="
  background: #E7F3FF;
  border-left: 4px solid #0668E1;
  padding: 20px 24px;
  border-radius: 0 12px 12px 0;
  margin: 24px 0;
">
  <strong style="color: #0668E1;">üí° Key Insight</strong>
  <p style="margin: 8px 0 0 0; color: #1C1E21;">
    Using a pre-grasp pose 5cm above the target significantly improved grasp success by allowing the controller to correct for small positioning errors during the final approach.
  </p>
</div>
<hr>
<h2 id="-results">üìä Results<a hidden class="anchor" aria-hidden="true" href="#-results">#</a></h2>
<p>Our system achieved the following performance metrics:</p>
<table>
  <thead>
      <tr>
          <th>Metric</th>
          <th>Value</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Grasp Success Rate</td>
          <td>95.2%</td>
      </tr>
      <tr>
          <td>Average Cycle Time</td>
          <td>8.3 seconds</td>
      </tr>
      <tr>
          <td>Position Accuracy</td>
          <td>¬±2.1 mm</td>
      </tr>
      <tr>
          <td>Orientation Error</td>
          <td>¬±3.5¬∞</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="-demo-video">üé¨ Demo Video<a hidden class="anchor" aria-hidden="true" href="#-demo-video">#</a></h2>
<div style="
  background: #ffffff;
  border: 1px solid #E4E6EB;
  border-radius: 16px;
  overflow: hidden;
  box-shadow: 0 4px 20px rgba(0,0,0,0.08);
  margin: 32px 0;
">
  <div style="position: relative; padding-bottom: 56.25%; height: 0;">
    <iframe 
      style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border: none;"
      src="https://www.youtube.com/embed/dQw4w9WgXcQ"
      title="Project Demo"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
      allowfullscreen>
    </iframe>
  </div>
</div>
<hr>
<h2 id="-resources">üîó Resources<a hidden class="anchor" aria-hidden="true" href="#-resources">#</a></h2>
<div style="
  display: flex;
  gap: 16px;
  flex-wrap: wrap;
  margin: 24px 0;
">
  <a href="#" style="
    display: inline-flex;
    align-items: center;
    gap: 8px;
    background: #1C1E21;
    color: white;
    padding: 12px 20px;
    border-radius: 8px;
    font-weight: 600;
    font-size: 0.9375rem;
    text-decoration: none;
  ">
    üìÅ GitHub Repository
  </a>
  <a href="#" style="
    display: inline-flex;
    align-items: center;
    gap: 8px;
    background: #0668E1;
    color: white;
    padding: 12px 20px;
    border-radius: 8px;
    font-weight: 600;
    font-size: 0.9375rem;
    text-decoration: none;
  ">
    üìÑ Project Report
  </a>
  <a href="#" style="
    display: inline-flex;
    align-items: center;
    gap: 8px;
    background: #E4E6EB;
    color: #1C1E21;
    padding: 12px 20px;
    border-radius: 8px;
    font-weight: 600;
    font-size: 0.9375rem;
    text-decoration: none;
  ">
    üéûÔ∏è Presentation Slides
  </a>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://berkeleydrone.github.io/tags/manipulation/">Manipulation</a></li>
      <li><a href="https://berkeleydrone.github.io/tags/computer-vision/">Computer-Vision</a></li>
      <li><a href="https://berkeleydrone.github.io/tags/ros2/">Ros2</a></li>
    </ul>

  </footer>
</article>
    </main>
    <footer style="
  background: #F8F9FA;
  border-top: 1px solid #E4E6EB;
  padding: 48px 24px;
  margin-top: -30px;
  width: 100vw;
  position: relative;
  left: 50%;
  right: 50%;
  margin-left: -50vw;
  margin-right: -50vw;
  z-index: 10;
">
  <div style="
    max-width: 1200px;
    margin: 0 auto;
    text-align: center;
  ">
    <div style="color: #8A8D91; font-size: 0.875rem; margin-bottom: 8px;">
      ¬© 2025 UC Berkeley ¬∑ EECS 106/206A Final Project
    </div>
    <div style="color: #BCC0C4; font-size: 0.8rem;">
      Built by <a href="https://dongc1.github.io/" target="_blank" style="color: #0668E1; text-decoration: none; font-weight: 500; transition: color 0.2s ease;" onmouseover="this.style.color='#5C7CFA'" onmouseout="this.style.color='#0668E1'">dongc1</a>
    </div>
  </div>
</footer>

<button class="back-to-top" id="backToTop">
  <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
    <path d="M12 4l-8 8h5v8h6v-8h5l-8-8z"/>
  </svg>
</button>

<script>
(function() {
  const backToTop = document.getElementById('backToTop');
  
  window.addEventListener('scroll', function() {
    if (window.scrollY > 300) {
      backToTop.classList.add('show');
    } else {
      backToTop.classList.remove('show');
    }
  });
  
  backToTop.addEventListener('click', function() {
    window.scrollTo({
      top: 0,
      behavior: 'smooth'
    });
  });
})();


const videoData = [
  {
    id: 'LBzI1GDyFOo',
    badge: 'Stage 1',
    title: 'Target Acquisition',
    desc: 'Initial marker detection and approach with aggressive gains for fast response. The drone quickly identifies the ArUco marker and begins its approach maneuver.',
    phase: 'Acquisition',
    control: 'High Gain PID',
    goal: 'Fast Detection'
  },
  {
    id: 'jF4QE1jB2JU',
    badge: 'Stage 2',
    title: 'Overshoot / Transient',
    desc: 'Tuning PID gains to reduce overshoot and oscillation during transient phase. This stage focuses on dampening the initial aggressive response.',
    phase: 'Transient',
    control: 'Dampened PID',
    goal: 'Reduce Oscillation'
  },
  {
    id: 'G2pCyJFHyck',
    badge: 'Stage 3',
    title: 'Target Tracking',
    desc: 'Stable hovering with smooth tracking. Low gains for minimal jitter. The drone maintains steady position relative to the marker.',
    phase: 'Tracking',
    control: 'Low Gain PID',
    goal: 'Smooth Tracking'
  },
  {
    id: 'WFyeybVqn1Q',
    badge: 'Stage 4',
    title: 'No Latency Tracking',
    desc: 'Optimized system with minimal latency for real-time responsive control. Final tuned parameters for competition-ready performance.',
    phase: 'Optimized',
    control: 'Tuned PID',
    goal: 'Zero Latency'
  }
];

function openModal(index) {
  const modal = document.getElementById('videoModal');
  const video = document.getElementById('modalVideo');
  const data = videoData[index];
  
  video.src = 'https://www.youtube.com/embed/' + data.id + '?autoplay=1';
  document.getElementById('modalBadge').textContent = data.badge;
  document.getElementById('modalTitle').textContent = data.title;
  document.getElementById('modalDesc').textContent = data.desc;
  document.getElementById('modalPhase').textContent = data.phase;
  document.getElementById('modalControl').textContent = data.control;
  document.getElementById('modalGoal').textContent = data.goal;
  
  modal.classList.add('active');
  document.body.style.overflow = 'hidden';
}

function closeModal(event) {
  const modal = document.getElementById('videoModal');
  const video = document.getElementById('modalVideo');
  
  video.src = '';
  modal.classList.remove('active');
  document.body.style.overflow = '';
}

document.addEventListener('keydown', function(e) {
  if (e.key === 'Escape') {
    closeModal();
    closeDocModal();
  }
});


const docData = [
  {
    tag: 'Hardware',
    title: 'Drone Frame & Components',
    subtitle: 'A2RL Racing Drone Platform',
    bg: 'https://berkeleydrone.github.io/images/docs/drone_parts.png',
    content: `
      <h3>Overview</h3>
      <p>In this project, the Jetson Xavier is used as the main compute unit to run the code stack, including sensor drivers, control, perception, and planning modules. The communication is established between the Jetson Xavier (the brain) and the flight controller (the muscle).</p>
      
      <h3>Frame and Fasteners</h3>
      <table>
        <tr><th>Component</th><th>Type</th></tr>
        <tr><td>Frame</td><td>A2RL X DCL SPEC FRAME</td></tr>
        <tr><td>Battery Strap</td><td>Gemfan Kevlar Strap</td></tr>
        <tr><td>XT30 Connectors</td><td>XT30 Male and Female Pairs</td></tr>
      </table>
      
      <h3>Electronics</h3>
      <table>
        <tr><th>Component</th><th>Type</th></tr>
        <tr><td>Motors</td><td>XNOVA Black Thunder 2207 2100KV</td></tr>
        <tr><td>ESC</td><td>Foxeer Reaper F4 128K 65A BL32 4in1</td></tr>
        <tr><td>Flight Controller</td><td>Foxeer H7 MPU6000 FC</td></tr>
        <tr><td>VTX Antenna</td><td>Foxeer 5.8G Micro Lollipop</td></tr>
        <tr><td>VTX Camera</td><td>HDZERO Nano 90</td></tr>
        <tr><td>VTX</td><td>HDZERO Race 3 VTX</td></tr>
        <tr><td>Receiver</td><td>IRC Ghost Atto</td></tr>
      </table>
      
      <h3>RC Controller</h3>
      <table>
        <tr><th>Component</th><th>Type</th></tr>
        <tr><td>Controller</td><td>Radiomaster Zorro 4in1</td></tr>
        <tr><td>Transmitter Module</td><td>Ghost Module</td></tr>
        <tr><td>Propellers</td><td>MCK 51466 V2 (5 inch)</td></tr>
      </table>
      
      <img src="https://berkeleydrone.github.io/images/docs/drone_parts.png" alt="Drone Parts">
      <p class="img-caption">Figure 1: Parts and sensors used in the project</p>
      
      <img src="https://berkeleydrone.github.io/images/docs/Drone_frame_image.png" alt="Drone CAD">
      <p class="img-caption">Figure 2: CAD design of the drone parts</p>
    `
  },
  {
    tag: 'Calibration',
    title: 'Sensor Calibration',
    subtitle: 'Camera & IMU Calibration Procedures',
    bg: 'https://berkeleydrone.github.io/images/docs/calibrated_images.png',
    content: `
      <h3>Overview</h3>
      <p>Sensor calibration is an essential step for any real-world robotics platform to work properly and effectively. This is particularly true for custom-built platforms, such as the drone used in this project.</p>
      
      <h3>IMU Calibration</h3>
      <p>For the IMU calibration, we recorded an 11-hour static ROS 2 bag to estimate intrinsic noise characteristics and long-term drift. The bag was converted to ROS 1 format using the rosbags package to enable offline calibration with the Allan Variance toolkit.</p>
      <ul>
        <li>Initial IMU data rate: ~30 Hz</li>
        <li>Current maximum rate: 90 Hz</li>
        <li>Target rate: 200+ Hz for state estimation</li>
      </ul>
      
      <h3>Camera Calibration</h3>
      <p>Camera intrinsic calibration and IMU‚Äìcamera extrinsic calibration were performed using the Kalibr toolbox (ROS1). Multiple calibration bags were collected while moving the drone and checkerboard, producing accurate estimates of:</p>
      <ul>
        <li>Focal lengths and principal points</li>
        <li>Distortion parameters</li>
        <li>Rigid transformation T<sub>IC</sub> between IMU and camera frames</li>
      </ul>
      
      <img src="https://berkeleydrone.github.io/images/docs/calibrated_images.png" alt="Calibrated Images">
      <p class="img-caption">Figure 1: Demonstration of calibrated images</p>
      
      <img src="https://berkeleydrone.github.io/images/docs/accel_noise.png" alt="Accelerometer Noise">
      <p class="img-caption">Figure 2: Calibration results - accelerometer</p>
      
      <img src="https://berkeleydrone.github.io/images/docs/gyro_noise.png" alt="Gyroscope Noise">
      <p class="img-caption">Figure 3: Calibration results - gyroscope</p>
      
      <img src="https://berkeleydrone.github.io/images/docs/summary_noise.png" alt="Noise Summary">
      <p class="img-caption">Figure 4: Calibration results - summary</p>
    `
  },
  {
    tag: 'Perception',
    title: 'Perception Module',
    subtitle: 'ArUco Marker Detection & Tracking',
    bg: 'https://berkeleydrone.github.io/images/docs/pipeline_diagram.png',
    content: `
      <h3>Overview</h3>
      <p>The Perception Module is responsible for processing raw camera images to detect and localize visual targets (ArUco markers and gates) in the drone's environment. It acts as the sensory processing layer that bridges Hardware (camera sensors) and Planning (high-level decision making).</p>
      
      <h3>ArUco Detector Node</h3>
      <p>Detects ArUco markers in camera images and publishes their pixel coordinates and 3D pose in the camera frame.</p>
      <ul>
        <li><strong>Dictionary:</strong> DICT_4X4_50 (50 unique 4√ó4 markers)</li>
        <li><strong>Marker Size:</strong> 0.15 m (configurable)</li>
        <li><strong>Pose Estimation:</strong> cv2.solvePnP() with IPPE_SQUARE</li>
      </ul>
      
      <h3>Gate Detector Node</h3>
      <p>Detects racing gates using color-based segmentation (LAB color space) and contour detection with visual tracking (CSRT tracker).</p>
      <ul>
        <li><strong>Color Space:</strong> LAB for perceptually uniform segmentation</li>
        <li><strong>Tracking:</strong> CSRT (Discriminative Correlation Filter)</li>
        <li><strong>Gate Size:</strong> 1.4m √ó 1.4m</li>
      </ul>
      
      <h3>Data Flow</h3>
      <img src="https://berkeleydrone.github.io/images/docs/pipeline_diagram.png" alt="Pipeline Diagram">
      <p class="img-caption">ArUco Marker tracking pipeline diagram</p>
      
      <h3>Key Topics</h3>
      <table>
        <tr><th>Topic</th><th>Type</th><th>Description</th></tr>
        <tr><td>/aruco_detector/marker_pixel</td><td>PointStamped</td><td>Marker center in pixels</td></tr>
        <tr><td>/aruco_detector/marker_pose</td><td>PoseStamped</td><td>Marker 3D pose</td></tr>
        <tr><td>/aruco_detector/detected</td><td>Bool</td><td>Detection status</td></tr>
      </table>
    `
  },
  {
    tag: 'Planning',
    title: 'Planning Module',
    subtitle: 'Error Computation & Trajectory Planning',
    bg: 'https://berkeleydrone.github.io/images/docs/Drone_frame_image.png',
    content: `
      <h3>Overview</h3>
      <p>The Planning Module is responsible for generating desired drone poses (position, orientation, and velocity setpoints) based on pre-planned trajectories, sensor feedback, or real-time perception data.</p>
      
      <h3>Planning Strategies</h3>
      <ul>
        <li><strong>Autonomous Planner:</strong> Follows pre-recorded waypoint trajectories from CSV files using KD-tree nearest-neighbor search</li>
        <li><strong>ArUco Tracker Planner:</strong> Dynamically tracks ArUco markers, computing desired poses to keep markers centered</li>
        <li><strong>Sequential Planner:</strong> Simple waypoint-by-waypoint traversal without localization feedback</li>
        <li><strong>Manual Planner:</strong> Records pilot-controlled flight paths for later replay</li>
      </ul>
      
      <h3>ArUco Tracker Control Law</h3>
      <p>The tracker computes velocity commands to center the marker:</p>
      <ul>
        <li><code>error_x = marker_pixel.x - (image_width / 2)</code></li>
        <li><code>vel_y = -gain_x √ó error_x</code> (lateral velocity)</li>
        <li><code>vel_x = gain_z √ó distance_error</code> (forward velocity)</li>
      </ul>
      
      <h3>Configuration Parameters</h3>
      <table>
        <tr><th>Parameter</th><th>Default</th><th>Description</th></tr>
        <tr><td>image_width/height</td><td>1920√ó1080</td><td>Camera resolution</td></tr>
        <tr><td>gain_x, gain_y</td><td>0.002</td><td>Pixel to velocity gain</td></tr>
        <tr><td>gain_z</td><td>0.5</td><td>Distance to velocity gain</td></tr>
        <tr><td>target_distance</td><td>1.5 m</td><td>Desired distance to marker</td></tr>
        <tr><td>max_vel_xy</td><td>0.5 m/s</td><td>Max lateral velocity</td></tr>
      </table>
    `
  },
  {
    tag: 'Control',
    title: 'Control Module',
    subtitle: 'PID Controller & Velocity Commands',
    bg: 'https://berkeleydrone.github.io/images/docs/accel_noise.png',
    content: `
      <h3>Overview</h3>
      <p>The Control Module is responsible for converting high-level desired poses and velocities (from the Planning Module) into low-level RC commands that drive the drone's motors via the flight controller.</p>
      
      <h3>Control Strategies</h3>
      <ul>
        <li><strong>Autonomous Control Node:</strong> Full 6-DOF position and attitude control using cascade PID with localization feedback</li>
        <li><strong>Velocity Control Node:</strong> Simplified velocity-to-RC mapping for visual servoing (no localization required)</li>
        <li><strong>Manual Control Node:</strong> Pass-through relay for human pilot RC commands</li>
      </ul>
      
      <h3>PID Configuration</h3>
      <table>
        <tr><th>Axis</th><th>KP</th><th>KI</th><th>KD</th></tr>
        <tr><td>X-axis</td><td>8.0</td><td>0.0</td><td>5.0</td></tr>
        <tr><td>Y-axis</td><td>8.0</td><td>0.0</td><td>5.0</td></tr>
        <tr><td>Z-axis (altitude)</td><td>20.0</td><td>0.0</td><td>5.0</td></tr>
        <tr><td>Yaw</td><td>1.0</td><td>0.0</td><td>0.0</td></tr>
      </table>
      
      <h3>Control Law</h3>
      <p><strong>Outer Loop (Position Control):</strong></p>
      <ul>
        <li><code>pos_error = desired_pos - current_pos</code></li>
        <li><code>r_ddot_des = KP √ó pos_error + KD √ó vel_error</code></li>
      </ul>
      
      <p><strong>Inner Loop (Attitude):</strong></p>
      <ul>
        <li><code>phi_des (roll) = (r_ddot_des_x √ó sin(yaw) - r_ddot_des_y √ó cos(yaw)) / g</code></li>
        <li><code>theta_des (pitch) = (r_ddot_des_x √ó cos(yaw) + r_ddot_des_y √ó sin(yaw)) / g</code></li>
      </ul>
      
      <h3>PWM Channel Mapping</h3>
      <table>
        <tr><th>Channel</th><th>Function</th><th>Range</th></tr>
        <tr><td>1</td><td>Roll</td><td>1000-2000 PWM</td></tr>
        <tr><td>2</td><td>Pitch</td><td>1000-2000 PWM</td></tr>
        <tr><td>3</td><td>Throttle</td><td>1000-2000 PWM</td></tr>
        <tr><td>4</td><td>Yaw</td><td>1000-2000 PWM</td></tr>
        <tr><td>5</td><td>AUX1 (ARM)</td><td>1000 = armed</td></tr>
      </table>
    `
  },
  {
    tag: 'Workflow',
    title: 'Development Workflow',
    subtitle: 'Software Development & Debugging Guide',
    bg: 'https://berkeleydrone.github.io/images/docs/gyro_noise.png',
    content: `
      <h3>Overview</h3>
      <p>This document summarizes the development workflow in three areas: hardware setup and testing, sensor calibration, and task implementation.</p>
      
      <h3>Step 1: Hardware Setup and Testing</h3>
      <p>We assembled and bench-tested the aircraft powertrain, flight controller, radios, and data links.</p>
      <ul>
        <li><strong>Visual inspection:</strong> Frame integrity, motor/prop orientation, connector seating</li>
        <li><strong>Power checks:</strong> Battery voltage, PDU rail voltages</li>
        <li><strong>ESC/motor bench test:</strong> Spin each motor at low throttle (props removed)</li>
        <li><strong>Telemetry and RC link test:</strong> Verify RC receiver channels and ground station connectivity</li>
      </ul>
      
      <h3>Step 2: Sensor Calibration</h3>
      <p>We calibrated IMU, magnetometer, barometer, and camera to ensure reliable state estimation.</p>
      <ul>
        <li><strong>IMU calibration:</strong> Static bias, temperature compensation tables</li>
        <li><strong>Magnetometer:</strong> Full rotation / figure-eight motions for hard/soft iron corrections</li>
        <li><strong>Camera:</strong> OpenCV calibrateCamera for intrinsics and distortion</li>
      </ul>
      
      <h3>Step 3: Task Implementation</h3>
      <p>With hardware validated and sensors calibrated, we implemented mission-level tasks.</p>
      <ul>
        <li><strong>State estimation integration:</strong> EKF/UKF with sensor streams</li>
        <li><strong>Control loop tuning:</strong> PID/P gains using step responses</li>
        <li><strong>Mission scripting:</strong> YAML/JSON waypoint definitions</li>
        <li><strong>Testing progression:</strong> SITL ‚Üí bench tests ‚Üí tethered flights ‚Üí real flights</li>
      </ul>
      
      <img src="https://berkeleydrone.github.io/images/docs/pipeline_diagram.png" alt="Pipeline Diagram">
      <p class="img-caption">ArUco Marker tracking pipeline diagram</p>
      
      <h3>Recommended Tools</h3>
      <table>
        <tr><th>Tool</th><th>Purpose</th></tr>
        <tr><td>Gazebo</td><td>Physics simulation with wind and sensor noise</td></tr>
        <tr><td>MAVProxy / QGroundControl</td><td>Ground station monitoring</td></tr>
        <tr><td>rosbag</td><td>Data recording and replay</td></tr>
        <tr><td>OpenCV / NumPy / SciPy</td><td>Calibration and image processing</td></tr>
      </table>
    `
  }
];

function openDocModal(index) {
  const modal = document.getElementById('docModal');
  const data = docData[index];

  document.getElementById('docModalTag').textContent = data.tag;
  document.getElementById('docModalTitle').textContent = data.title;
  document.getElementById('docModalSubtitle').textContent = data.subtitle || 'Technical Documentation';
  document.getElementById('docModalImage').src = data.bg;
  document.getElementById('docModalImage').alt = data.title;
  document.getElementById('docModalContent').innerHTML = data.content;

  modal.classList.add('active');
  document.body.style.overflow = 'hidden';
}

function closeDocModal(event) {
  const modal = document.getElementById('docModal');
  modal.classList.remove('active');
  document.body.style.overflow = '';
}
</script>
</body>

</html>
